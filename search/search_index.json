{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K8S-GitOps \u00b6 This repository serves as the declarative \"source of truth\" for my Kubernetes cluster at home. Flux watches over the cluster folder to check for changes and apply them to the cluster. The k8s-at-home/template-cluster-k3s repository served as a loose base for things.","title":"Home"},{"location":"#k8s-gitops","text":"This repository serves as the declarative \"source of truth\" for my Kubernetes cluster at home. Flux watches over the cluster folder to check for changes and apply them to the cluster. The k8s-at-home/template-cluster-k3s repository served as a loose base for things.","title":"K8S-GitOps"},{"location":"bootstrap/","text":"Bootstrap \u00b6 Create an age key \u00b6 age-keygen -o age.agekey Verify if the cluster is ready for Flux \u00b6 flux check --pre Create the flux-system namespace \u00b6 kubectl create namespace flux-system Add the AGE key for decrypting secrets \u00b6 cat age.agekey | kubectl -n flux-system create secret generic sops-age \\ --from-file = age.agekey = /dev/stdin Install Flux \u00b6 !!! warning \"You should run this command twice because there are race conditions on the Flux CRDs. The second run should work without errors. kubectl apply --kustomize = ./cluster/base/flux-system At this point, Flux should start reconciling the cluster into the desired state.","title":"Bootstrap"},{"location":"bootstrap/#bootstrap","text":"","title":"Bootstrap"},{"location":"bootstrap/#create-an-age-key","text":"age-keygen -o age.agekey","title":"Create an age key"},{"location":"bootstrap/#verify-if-the-cluster-is-ready-for-flux","text":"flux check --pre","title":"Verify if the cluster is ready for Flux"},{"location":"bootstrap/#create-the-flux-system-namespace","text":"kubectl create namespace flux-system","title":"Create the flux-system namespace"},{"location":"bootstrap/#add-the-age-key-for-decrypting-secrets","text":"cat age.agekey | kubectl -n flux-system create secret generic sops-age \\ --from-file = age.agekey = /dev/stdin","title":"Add the AGE key for decrypting secrets"},{"location":"bootstrap/#install-flux","text":"!!! warning \"You should run this command twice because there are race conditions on the Flux CRDs. The second run should work without errors. kubectl apply --kustomize = ./cluster/base/flux-system At this point, Flux should start reconciling the cluster into the desired state.","title":"Install Flux"},{"location":"overview/","text":"Detailed overview \u00b6 Cluster setup \u00b6 The cluster runs k3s on top of Ubuntu 21.04 using the Ansible role from ansible-role-k3s . It runs semi-hyperconverged, with storage and workloads sharing machines and NFS on a NAS for larger files. Components \u00b6 Calico : for internal networking for the cluster using BGP. Rook : Persistent volumes using Ceph RBD storage. Mozilla SOPS : Encryption of secrets. external-dns : Create DNS entries in a coredns deployment for use outside the cluster. cert-manager : Provides the TLS certificates for the ingresses. kube-vip : HA for the control plane, inside the cluster. Structure \u00b6 Under the cluster directory, the following structure is present for flux to apply: base is the entrypoint for Flux crds contains the custom resource definitions (CRDs) that need to exist for other parts of the cluster to exist core has all the important core components that should be present, such as storage, with a dependency on crds apps contains the applications that run inside the cluster, with a dependency on core Automation \u00b6 Github actions check for code formatting, build docs and some other very important background tasks on Git changes. System Upgrade Controller will automatically update k3s. Kured automatically reboots the nodes whenever reboots are required for OS updates. Renovate will automatically open pull requests for new versions of Helm charts and container images, with some help from the k8s-at-home Github action for annotations.","title":"Detailed overview"},{"location":"overview/#detailed-overview","text":"","title":"Detailed overview"},{"location":"overview/#cluster-setup","text":"The cluster runs k3s on top of Ubuntu 21.04 using the Ansible role from ansible-role-k3s . It runs semi-hyperconverged, with storage and workloads sharing machines and NFS on a NAS for larger files.","title":"Cluster setup"},{"location":"overview/#components","text":"Calico : for internal networking for the cluster using BGP. Rook : Persistent volumes using Ceph RBD storage. Mozilla SOPS : Encryption of secrets. external-dns : Create DNS entries in a coredns deployment for use outside the cluster. cert-manager : Provides the TLS certificates for the ingresses. kube-vip : HA for the control plane, inside the cluster.","title":"Components"},{"location":"overview/#structure","text":"Under the cluster directory, the following structure is present for flux to apply: base is the entrypoint for Flux crds contains the custom resource definitions (CRDs) that need to exist for other parts of the cluster to exist core has all the important core components that should be present, such as storage, with a dependency on crds apps contains the applications that run inside the cluster, with a dependency on core","title":"Structure"},{"location":"overview/#automation","text":"Github actions check for code formatting, build docs and some other very important background tasks on Git changes. System Upgrade Controller will automatically update k3s. Kured automatically reboots the nodes whenever reboots are required for OS updates. Renovate will automatically open pull requests for new versions of Helm charts and container images, with some help from the k8s-at-home Github action for annotations.","title":"Automation"},{"location":"rook/","text":"Rook \u00b6 Directly interacting with the Ceph cluster \u00b6 Open up a shell in the rook-ceph-direct-mount pod. kubectl exec -it -n rook-ceph rook-direct-mount-5fd4b8d8c5-8sd9q -- bash In this shell, all Ceph commands can be executed. Directly mounting a volume \u00b6 First, suspend the HelmRelease and scale down the deployment so the storage is not in use. flux suspend helmrelease -n media bazarr kubectl scale deployment -n media bazarr --replicas 0 Next step is extracting the csi-vol-* string from the PersistentVolume. kubectl get pv/ $( kubectl get pv | grep bazarr-config \\ | awk -F ' ' '{print $1}' ) -n home -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' After this, connect to the rook-ceph-direct-mount pod, create a mount path and map the volume. mkdir -p /mnt/data rbd map -p replicapool csi-vol-d72a57f3-1370-11ec-aa8b-babccd65caf2 \\ | xargs -I {} mount {} /mnt/data Afterwards, unmount and unmap the volume. umount /mnt/data rbd unmap -p replicapool csi-vol-d72a57f3-1370-11ec-aa8b-babccd65caf2 Correcting cluster health issues \u00b6 Whenever a health issue occurs, usually the solution and/or troubleshooting steps can be found in the Ceph docs .","title":"Rook"},{"location":"rook/#rook","text":"","title":"Rook"},{"location":"rook/#directly-interacting-with-the-ceph-cluster","text":"Open up a shell in the rook-ceph-direct-mount pod. kubectl exec -it -n rook-ceph rook-direct-mount-5fd4b8d8c5-8sd9q -- bash In this shell, all Ceph commands can be executed.","title":"Directly interacting with the Ceph cluster"},{"location":"rook/#directly-mounting-a-volume","text":"First, suspend the HelmRelease and scale down the deployment so the storage is not in use. flux suspend helmrelease -n media bazarr kubectl scale deployment -n media bazarr --replicas 0 Next step is extracting the csi-vol-* string from the PersistentVolume. kubectl get pv/ $( kubectl get pv | grep bazarr-config \\ | awk -F ' ' '{print $1}' ) -n home -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' After this, connect to the rook-ceph-direct-mount pod, create a mount path and map the volume. mkdir -p /mnt/data rbd map -p replicapool csi-vol-d72a57f3-1370-11ec-aa8b-babccd65caf2 \\ | xargs -I {} mount {} /mnt/data Afterwards, unmount and unmap the volume. umount /mnt/data rbd unmap -p replicapool csi-vol-d72a57f3-1370-11ec-aa8b-babccd65caf2","title":"Directly mounting a volume"},{"location":"rook/#correcting-cluster-health-issues","text":"Whenever a health issue occurs, usually the solution and/or troubleshooting steps can be found in the Ceph docs .","title":"Correcting cluster health issues"}]}